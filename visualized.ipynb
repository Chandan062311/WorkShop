{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Visualization Notebook\n",
    "This notebook demonstrates supervised and unsupervised learning models with graphical visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_blobs, make_regression, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def plot_classification(X, y, model, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, edgecolor='k')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_classes=2, n_clusters_per_class=1, n_samples=300, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "models = {\"Decision Tree\": DecisionTreeClassifier(),\n",
    "          \"SVM\": SVC(),\n",
    "          \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "          \"Random Forest\": RandomForestClassifier(n_estimators=10),\n",
    "          \"Logistic Regression\": LogisticRegression(),\n",
    "          \"Naive Bayes\": GaussianNB(),\n",
    "          \"Gradient Boosting\": GradientBoostingClassifier()}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    plot_classification(X, y, model, f'{name} Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "lin_reg = LinearRegression().fit(X, y)\n",
    "y_pred = lin_reg.predict(X)\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "clustering_models = {\"K-Means\": KMeans(n_clusters=3),\n",
    "                    \"Hierarchical Clustering\": AgglomerativeClustering(n_clusters=3),\n",
    "                    \"DBSCAN\": DBSCAN(eps=0.5)}\n",
    "for name, model in clustering_models.items():\n",
    "    model.fit(X)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap='viridis')\n",
    "    plt.title(f'{name} Clustering')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: PCA & Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, n_features=3)\n",
    "pca = PCA(n_components=2).fit_transform(X)\n",
    "plt.scatter(pca[:, 0], pca[:, 1], cmap='coolwarm')\n",
    "plt.title(\"PCA Dimensionality Reduction\")\n",
    "plt.show()\n",
    "gmm = GaussianMixture(n_components=3).fit(X)\n",
    "labels = gmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm')\n",
    "plt.title(\"Gaussian Mixture Model Clustering\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
