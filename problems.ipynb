{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-story",
   "metadata": {},
   "source": [
    "# A Data Cleaning Story: Overcoming Data Engineering Challenges\n",
    "\n",
    "Welcome to our story of data cleaning! Imagine you're a data engineer embarking on an adventure to tame a wild, messy dataset. Throughout this journey, you'll face common challenges—like missing values, outliers, duplicates, and more. Let's explore these challenges and learn how to solve them using Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-1-missing-values",
   "metadata": {},
   "source": [
    "## Chapter 1: The Case of the Missing Values\n",
    "\n",
    "Our first challenge is missing values. In our dataset, some rows have gaps where data should be. This might happen due to errors in data collection or transmission.\n",
    "\n",
    "Let's see how we can identify and handle these missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-missing-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simulate a small dataset with missing values\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', None],\n",
    "    'Age': [25, np.nan, 35, 40, 30],\n",
    "    'City': ['New York', 'Los Angeles', None, 'Houston', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print('Original DataFrame:')\n",
    "print(df)\n",
    "\n",
    "# Identify missing values\n",
    "print('\\nMissing values in each column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Option 1: Drop rows with any missing values\n",
    "df_dropped = df.dropna()\n",
    "print('\\nDataFrame after dropping missing values:')\n",
    "print(df_dropped)\n",
    "\n",
    "# Option 2: Impute missing values\n",
    "df_imputed = df.copy()\n",
    "df_imputed['Age'] = df_imputed['Age'].fillna(df_imputed['Age'].mean())\n",
    "df_imputed['City'] = df_imputed['City'].fillna('Unknown')\n",
    "df_imputed['Name'] = df_imputed['Name'].fillna('Unknown')\n",
    "print('\\nDataFrame after imputing missing values:')\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-2-outliers",
   "metadata": {},
   "source": [
    "## Chapter 2: Battling the Outliers\n",
    "\n",
    "Our next challenge is dealing with outliers. Outliers are data points that are far from the majority of our data. They can distort our analysis and affect model performance.\n",
    "\n",
    "Let's simulate a dataset and see how we can detect and handle outliers using the Interquartile Range (IQR) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate data with an outlier\n",
    "np.random.seed(0)\n",
    "data = np.random.normal(50, 5, 100).tolist()\n",
    "data.append(100)  # Introduce an outlier\n",
    "df_outliers = pd.DataFrame({'Value': data})\n",
    "\n",
    "# Plot boxplot to visualize outliers\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.boxplot(df_outliers['Value'], vert=False)\n",
    "plt.title('Boxplot of Values with Outlier')\n",
    "plt.xlabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# Identify outliers using IQR\n",
    "Q1 = df_outliers['Value'].quantile(0.25)\n",
    "Q3 = df_outliers['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print('Lower Bound:', lower_bound, 'Upper Bound:', upper_bound)\n",
    "\n",
    "# Filter out outliers\n",
    "df_no_outliers = df_outliers[(df_outliers['Value'] >= lower_bound) & (df_outliers['Value'] <= upper_bound)]\n",
    "print('\\nData without outliers:')\n",
    "print(df_no_outliers.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-3-duplicates",
   "metadata": {},
   "source": [
    "## Chapter 3: The Duplicate Dilemma\n",
    "\n",
    "Sometimes, duplicate records sneak into our dataset—perhaps due to data entry errors or merging multiple sources. Duplicate rows can skew our analysis and lead to erroneous insights.\n",
    "\n",
    "Let's detect and remove duplicates from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a dataset with duplicates\n",
    "data_dup = {\n",
    "    'ID': [1, 2, 2, 3, 4, 4, 4],\n",
    "    'Value': [10, 20, 20, 30, 40, 40, 40]\n",
    "}\n",
    "df_dup = pd.DataFrame(data_dup)\n",
    "print('Original DataFrame with Duplicates:')\n",
    "print(df_dup)\n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicates = df_dup[df_dup.duplicated()]\n",
    "print('\\nDuplicate Rows:')\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates\n",
    "df_unique = df_dup.drop_duplicates()\n",
    "print('\\nDataFrame after removing duplicates:')\n",
    "print(df_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-4-inconsistent-formatting",
   "metadata": {},
   "source": [
    "## Chapter 4: Inconsistent Data Formatting\n",
    "\n",
    "In our adventure, we also encounter data that's formatted in different ways. For example, dates might appear in multiple formats, or text data might have inconsistent capitalization.\n",
    "\n",
    "Let's see how we can standardize data formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-formatting",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format = {\n",
    "    'Date': ['2023/01/01', '01-02-2023', '2023.03.01'],\n",
    "    'City': ['new york', 'LOS ANGELES', 'ChIcAgO']\n",
    "}\n",
    "df_format = pd.DataFrame(data_format)\n",
    "print('Original Data Formatting:')\n",
    "print(df_format)\n",
    "\n",
    "# Standardize date format\n",
    "df_format['Date'] = pd.to_datetime(df_format['Date'], errors='coerce')\n",
    "\n",
    "# Standardize text data (capitalize city names)\n",
    "df_format['City'] = df_format['City'].str.title()\n",
    "print('\\nStandardized Data Formatting:')\n",
    "print(df_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-5-datatypes",
   "metadata": {},
   "source": [
    "## Chapter 5: Fixing Incorrect Data Types\n",
    "\n",
    "At times, columns may have the wrong data type. For instance, numerical values might be stored as strings. This can cause problems during analysis.\n",
    "\n",
    "Let's correct these data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-datatypes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data with incorrect data types\n",
    "data_types = {\n",
    "    'ID': ['1', '2', '3', '4'],\n",
    "    'Sales': ['100.5', '200.75', '150.25', '300.0']\n",
    "}\n",
    "df_types = pd.DataFrame(data_types)\n",
    "print('Data with Incorrect Types:')\n",
    "print(df_types.dtypes)\n",
    "\n",
    "# Convert columns to the correct data type\n",
    "df_types['ID'] = df_types['ID'].astype(int)\n",
    "df_types['Sales'] = pd.to_numeric(df_types['Sales'], errors='coerce')\n",
    "print('\\nData Types after Conversion:')\n",
    "print(df_types.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-6-noise",
   "metadata": {},
   "source": [
    "## Chapter 6: Silencing the Noise and Data Errors\n",
    "\n",
    "Our data journey might also be marred by noisy data—typos, extra spaces, or irrelevant entries. Noise can lead to inaccurate analysis if not handled properly.\n",
    "\n",
    "Let's clean up some textual noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate noisy text data\n",
    "data_noise = {\n",
    "    'Product': ['  Apple ', 'banana', 'Cherry', 'APPLE', 'BaNaNa ']\n",
    "}\n",
    "df_noise = pd.DataFrame(data_noise)\n",
    "print('Original Noisy Data:')\n",
    "print(df_noise)\n",
    "\n",
    "# Clean the text: strip extra spaces and convert to lowercase\n",
    "df_noise['Product'] = df_noise['Product'].str.strip().str.lower()\n",
    "print('\\nCleaned Text Data:')\n",
    "print(df_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-7-integration",
   "metadata": {},
   "source": [
    "## Chapter 7: Integrating Data from Different Sources\n",
    "\n",
    "Sometimes, you need to merge data from different sources. This can be tricky if the keys or schemas don't match perfectly.\n",
    "\n",
    "Let's simulate merging two datasets with a common key after aligning their formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate two datasets\n",
    "data1 = {\n",
    "    'ID': [1, 2, 3],\n",
    "    'Value1': [10, 20, 30]\n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    'id': ['1', '2', '4'],\n",
    "    'Value2': [100, 200, 400]\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Align key column names and data types\n",
    "df2.rename(columns={'id': 'ID'}, inplace=True)\n",
    "df2['ID'] = df2['ID'].astype(int)\n",
    "\n",
    "# Merge the datasets on the common key\n",
    "merged_df = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print('Merged DataFrame:')\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chapter-8-high-cardinality",
   "metadata": {},
   "source": [
    "## Chapter 8: Taming High Cardinality in Categorical Data\n",
    "\n",
    "High cardinality can be overwhelming when there are too many unique values in a categorical feature. It can complicate analysis and modeling.\n",
    "\n",
    "A common strategy is to group infrequent categories into an \"Other\" category. Let's see how it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-high-cardinality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate categorical data with high cardinality\n",
    "data_card = {\n",
    "    'Category': ['A', 'B', 'C', 'A', 'B', 'D', 'E', 'F', 'G', 'B', 'C', 'H']\n",
    "}\n",
    "df_card = pd.DataFrame(data_card)\n",
    "print('Original Categories:')\n",
    "print(df_card['Category'].value_counts())\n",
    "\n",
    "# Group categories that appear less than 2 times into 'Other'\n",
    "threshold = 2\n",
    "counts = df_card['Category'].value_counts()\n",
    "to_keep = counts[counts >= threshold].index\n",
    "df_card['Category'] = df_card['Category'].apply(lambda x: x if x in to_keep else 'Other')\n",
    "print('\\nCategories after grouping infrequent ones:')\n",
    "print(df_card['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-story",
   "metadata": {},
   "source": [
    "## Conclusion: The Journey Continues\n",
    "\n",
    "In this data cleaning story, we've encountered and overcome several common challenges:\n",
    "\n",
    "- **Missing Values:** Identified and imputed or removed missing data.\n",
    "- **Outliers:** Detected and treated outliers using statistical methods.\n",
    "- **Duplicates:** Found and removed duplicate records.\n",
    "- **Inconsistent Formatting:** Standardized dates and text to maintain uniformity.\n",
    "- **Incorrect Data Types:** Converted data to the correct types for accurate analysis.\n",
    "- **Noise:** Cleaned textual data to reduce errors and inconsistencies.\n",
    "- **Data Integration:** Merged datasets after aligning key columns and formats.\n",
    "- **High Cardinality:** Grouped infrequent categories to simplify analysis.\n",
    "\n",
    "Each of these steps is essential to ensure that the data we work with is clean, consistent, and ready for further analysis or machine learning. Remember, data cleaning is an iterative process—each dataset is unique and may require custom solutions.\n",
    "\n",
    "Keep exploring, and happy data cleaning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
